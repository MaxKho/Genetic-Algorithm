# -*- coding: utf-8 -*-
"""Best_net.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eBPMfrmcUzOQA-xa3owwARPEs3Zr3TB
"""

import os
import torch
import librosa
import numpy as np
import scipy.io
from scipy.signal.windows import hann
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
from sklearn.model_selection import train_test_split


# Use the best architecture found by the genetic algorithm
BATCH_SIZE = 16
LR = 0.001
EPOCHS = 8
class Net(nn.Module):
    def __init__(self, D=3, K=4, input_channels=1):
        super().__init__()
        layers = []
        genome =  [{'conv_filters': 32, 'kernel_size': 5, 'stride': 1, 'padding': 0, 'pool_size': 2, 'activation': 'relu'},
                  {'conv_filters': 16, 'kernel_size': 7, 'stride': 2, 'padding': 1, 'pool_size': 3, 'activation': 'relu'},
                  {'conv_filters': 64, 'kernel_size': 5, 'stride': 1, 'padding': 1, 'pool_size': 2, 'activation': 'swish'}, {'conv_filters': 16, 'kernel_size': 3, 'stride': 1, 'padding': 0, 'pool_size': 2, 'activation': 'swish'},
                  {'neurons': 128, 'activation': 'relu', 'dropout': 0.1},
                  {'neurons': 32, 'activation': 'sigmoid', 'dropout': 0.1}]


        conv_output_size = D
        activation_map = {
            'sigmoid': nn.Sigmoid(),
            'relu': nn.ReLU(),
            'leaky_relu': nn.LeakyReLU(),
            'swish': nn.SiLU()}

        for gene in genome:
            if 'conv_filters' in gene:
                gene['kernel_size'] = min([gene['kernel_size'], conv_output_size])
                gene['stride'] = min([gene['stride'], max([1, conv_output_size // 2])])

                conv_output_size = (conv_output_size - gene['kernel_size'] + 2 * gene['padding']) // gene['stride'] + 1
                gene['pool_size'] = max([1, min([gene['pool_size'], conv_output_size])])
                conv_output_size = conv_output_size // gene['pool_size']
                conv_output_size = max(1, conv_output_size)

                layers.append(nn.Conv1d(
                    in_channels=input_channels,
                    out_channels=gene['conv_filters'],
                    kernel_size=gene['kernel_size'],
                    stride=gene['stride'],
                    padding=gene['padding']
                ))
                layers.append(activation_map[gene['activation']])
                layers.append(nn.MaxPool1d(kernel_size=gene['pool_size']))
                input_channels = gene['conv_filters']

        layers.append(nn.Flatten())
        fc_input_size = input_channels * conv_output_size

        for gene in genome:
            if 'neurons' in gene:
                layers.append(nn.Linear(fc_input_size, gene['neurons']))
                layers.append(activation_map[gene['activation']])
                layers.append(nn.Dropout(gene['dropout']))
                fc_input_size = gene['neurons']

        layers.append(nn.Linear(fc_input_size, K))
        self.network = nn.Sequential(*layers)


    def forward(self, x):
        """
        Defines the forward pass of the neural network.

        Parameters:
        - x (Tensor): The input data tensor.

        Returns:
        - Tensor: The output of the network after processing the input tensor through all the layers defined
          in the `network` attribute.
        """
        if x.dim() == 2:
            x = x.unsqueeze(1)
        return self.network(x)


def load_and_process_val_pcvc_data(directory='.', train_size=0.8, random_seed=42):

    # List all .mat files in the specified directory
    all_mats = [file for file in os.listdir(directory) if file.endswith('.mat')]
    raw_data = []
    num_vowels = 6
    ndatapoints_per_vowel = 299
    labels = []

    for idx, mat_file in enumerate(all_mats):
        mat_path = os.path.join(directory, mat_file)
        mat_data = np.squeeze(scipy.io.loadmat(mat_path)['x'])
        raw_data.append(mat_data)
        labels.append(np.repeat(np.arange(num_vowels)[np.newaxis], mat_data.shape[0], axis=0))

    # Concatenate and reshape all data
    raw_data, labels = np.concatenate(raw_data, axis=1), np.concatenate(labels, axis=1)
    nreps, nvow, nsamps = raw_data.shape
    raw_data = np.reshape(raw_data, (nreps * nvow, nsamps), order='F')
    labels = np.reshape(labels, (nreps * nvow), order = 'F')

    # Split data into training and validation sets
    tr_data, vl_data, tr_labels, vl_labels = train_test_split(
        raw_data, labels, train_size=train_size, random_state=random_seed, stratify=labels)

    # Define window size and function
    window_size = 10000
    window = hann(window_size)

    # Process Validation Data with fixed slicing
    vl_data = vl_data[:, 5000:15000] * window
    vl_data = np.array([librosa.resample(d, orig_sr=48000, target_sr=16000) for d in vl_data])

    # One-hot encode labels
    vl_labels = np.eye(num_vowels)[vl_labels]

    return vl_data, vl_labels.astype('float')

# Load and process the PCVC dataset
X_val, labels_val = load_and_process_val_pcvc_data()
Nsamps, Nclasses = X_val.shape[-1], labels_val.shape[-1]
X_val_tensor = torch.FloatTensor(X_val)
y_val_tensor = torch.FloatTensor(labels_val)
dataset_val = TensorDataset(X_val_tensor, y_val_tensor)
val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)

# Load the model
best_model = Net(D=Nsamps, K=Nclasses)  # Ensure the architecture matches
best_model.load_state_dict(torch.load('best_net.pth', weights_only=True,
                                      map_location=torch.device('cpu')))
print("Loaded the best model from 'best_net.pth'")

# Evaluate the model on the validation set
best_model.eval()
correct = 0
total = 0
with torch.no_grad():
    for data, target in val_loader:
        output = best_model(data)
        pred = output.argmax(dim=1, keepdim=True)
        target = target.argmax(dim=1, keepdim=True)
        correct += pred.eq(target).sum().item()
        total += target.size(0)

validation_accuracy = correct / total
print(f'Evaluation on validation set complete. Accuracy: {validation_accuracy:.4f} ({correct}/{total} correct)')